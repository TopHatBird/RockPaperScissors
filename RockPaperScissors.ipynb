{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RockPaperScissors.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "9XjVa5VP7HSM"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ltqvxqMZ2_PL"
      },
      "outputs": [],
      "source": [
        "valid_RPS_actions = [0, 1, 2] # Signifying Rock, Paper, or Scissors\n",
        "\n",
        "def playRPS(action_p1, action_p2):\n",
        "  if (action_p1 not in valid_RPS_actions) or (action_p2 not in valid_RPS_actions):\n",
        "    raise Exception(\"Invalid Move Detected.\")\n",
        "    return -100\n",
        "  \n",
        "  if action_p1 == action_p2: # If there is a draw, issue the agent a small penalty.\n",
        "    return (-2, 0)\n",
        "  \n",
        "  if (action_p1 == 0 and action_p2 == 1) or (action_p1 == 1 and action_p2 == 2) or (action_p1 == 2 and action_p2 == 0): # The ways the agent could lose against player 2.\n",
        "    return (-10, 10)\n",
        "  else:\n",
        "    return (10, -10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Agent (a.k.a. player 1 settings)\n",
        "agnt_hist = []\n",
        "\n",
        "pseudo_probs = [[333334, 333333, 333333]]\n",
        "\n",
        "# The agent essentially has 'pseudo probabilites' attached to the actions it can take.\n",
        "# These pseudo probabilies are adjusted through the incentives, but they are separate from the rewards.\n",
        "# The sum of the pseudo probabilities at any point should be the sum of the pseudo probabilities array.\n",
        "agnt_preferences = np.concatenate((np.array(valid_RPS_actions, ndmin=2), pseudo_probs), axis = 0)\n",
        "\n",
        "# This value affects the pseudo-probability distribution upon wins, losses, and draws.\n",
        "incentive_factor = 10;\n",
        "\n",
        "# Easier to use argmax by stuffing the eps_greedy method here.\n",
        "def eps_greedy(prob):\n",
        "  if prob >= random.random():\n",
        "    return random.randint(0, len(valid_RPS_actions) - 1)\n",
        "  else:\n",
        "    return np.argmax(agnt_preferences[1, :])\n",
        "\n",
        "# Player 2 Settings\n",
        "# p2_history = [] <-- May be important provided \n",
        "p2_preferences = [0] # A list that describes the possible options available to player 2. p2_pref = [0] means player 2 would only choose rock. [0, 2] would imply rock and scissors.\n",
        "adjusted_p2_preferences = [2] # This was used to swap p2's behavior part of the way through training.\n",
        "\n",
        "# Exploration rate of algorithm starting out when using epsilon greedy\n",
        "initial_greed_rate = 0.999\n",
        "running_gr = initial_greed_rate\n",
        "\n",
        "# Minimum value for exploration rate\n",
        "min_greed_rate = 0.0001\n",
        "\n",
        "# The number of sets of rounds of games that will be played.\n",
        "episodes = 10000\n",
        "\n",
        "# How many times to play the game per episode.\n",
        "rounds = 100\n",
        "\n",
        "for epis in range(episodes):\n",
        "  reward_total = 0\n",
        "  running_gr = 0.999 ** (epis + 1)\n",
        "\n",
        "  # After so many episodes, p2 changes its behavior.\n",
        "  if epis >= 2500:\n",
        "    p2_preferences = adjusted_p2_preferences\n",
        "\n",
        "  # The agent uses an epsilon greedy approach to pick its next move.\n",
        "  for rnds in range(rounds):\n",
        "    if running_gr >= min_greed_rate:\n",
        "      agnt_pick = eps_greedy(running_gr)\n",
        "    else:\n",
        "      agnt_pick = eps_greedy(min_greed_rate)\n",
        "    \n",
        "    # Player 2 picks randomly - A good future implementation would be to have it choose systematically.\n",
        "    p2_pick = random.choice(p2_preferences)\n",
        "    results = playRPS(agnt_pick, p2_pick)\n",
        "\n",
        "    # This section updates the agent's probabilities for selecting a winning action.\n",
        "\n",
        "    # Disincentivize losing actions.\n",
        "    if results == (-10, 10):\n",
        "      if agnt_pick == 0:\n",
        "        selection = random.choice([1, 2])\n",
        "        agnt_preferences[1, 0] = agnt_preferences[1, 0] - incentive_factor\n",
        "      elif agnt_pick == 1:\n",
        "        selection = random.choice([0, 2])\n",
        "        agnt_preferences[1, 1] = agnt_preferences[1, 1] - incentive_factor\n",
        "      elif agnt_pick == 2:\n",
        "        selection = random.choice([0, 1])\n",
        "        agnt_preferences[1, 2] = agnt_preferences[1, 2] - incentive_factor\n",
        "      else:\n",
        "        raise Exception(\"Invalid pick happened somewhere...\") \n",
        "        \n",
        "      agnt_preferences[1, selection] = agnt_preferences[1, selection] + incentive_factor\n",
        "\n",
        "    # Incentivize winning actions.\n",
        "    if results == (10, -10):\n",
        "      if agnt_pick == 0:\n",
        "        selection = random.choice([1, 2])\n",
        "        agnt_preferences[1, 0] = agnt_preferences[1, 0] + incentive_factor\n",
        "      elif agnt_pick == 1:\n",
        "        selection = random.choice([0, 2])\n",
        "        agnt_preferences[1, 1] = agnt_preferences[1, 1] + incentive_factor\n",
        "      elif agnt_pick == 2:\n",
        "        selection = random.choice([0, 1])\n",
        "        agnt_preferences[1, 2] = agnt_preferences[1, 2] + incentive_factor\n",
        "      else:\n",
        "        raise Exception(\"Invalid pick happened somewhere...\")\n",
        "\n",
        "      agnt_preferences[1, selection] = agnt_preferences[1, selection] - incentive_factor\n",
        "\n",
        "    # Disincentivize actions that lead to a draw.\n",
        "    if results == (-2, 0):\n",
        "      if p2_pick == 0:\n",
        "        selection = random.choice([1, 2])\n",
        "        agnt_preferences[1, 0] = agnt_preferences[1, 0] - incentive_factor\n",
        "      elif p2_pick == 1:\n",
        "        selection = random.choice([0, 2])\n",
        "        agnt_preferences[1, 1] = agnt_preferences[1, 1] - incentive_factor\n",
        "      elif p2_pick == 2:\n",
        "        selection = random.choice([0, 1])\n",
        "        agnt_preferences[1, 2] = agnt_preferences[1, 2] - incentive_factor\n",
        "      else:\n",
        "        raise Exception(\"Invalid pick happened somewhere...\")\n",
        "\n",
        "      agnt_preferences[1, selection] = agnt_preferences[1, selection] + incentive_factor\n",
        "\n",
        "    \n",
        "    reward_total += results[0]\n",
        "\n",
        "  agnt_hist.append(reward_total)\n",
        "  \n"
      ],
      "metadata": {
        "id": "yJqd0h_-7g1W"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analytics Section\n",
        "plt.plot(agnt_hist)\n",
        "plt.xlabel('Episode Number', fontsize = 24)\n",
        "plt.ylabel('Reward Value', fontsize = 24)\n",
        "plt.title('Trials with Rock-Paper-Scissors\\nReward Value vs. Episode\\n[ Opponent Picks Rock for Some Time then Scissors ]', fontsize = 24)\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(18.5, 10.5)\n",
        "print('Distribution of Agent Preferences\\n[Rock  Paper  Scissors]: ',agnt_preferences[1, :])"
      ],
      "metadata": {
        "id": "sQvuOd9WLozD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}